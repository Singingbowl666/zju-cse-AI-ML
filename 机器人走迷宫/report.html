<!DOCTYPE html><html><head>
      <title>report</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:///c:\Users\61583\.vscode\extensions\shd101wyy.markdown-preview-enhanced-0.8.18\crossnote\dependencies\katex\katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h2 id="center机器人走迷宫实验报告"><center>机器人走迷宫实验报告 </center></h2>
<h3 id="center-马涵柘-emspemsp-林祖鸿-emspemsp-杨志诚"><center> 马涵柘    林祖鸿    杨志诚 </center></h3>
<h3 id="一-实验目的">一、实验目的 </h3>
<p>使用python语言，使用深度优先搜索算法与Deep QLearning算法，完成机器人走迷宫。</p>
<p>1.理解并实现经典图搜索算法——深度优先搜索（DFS）。<br>
2.掌握强化学习中的Deep Q-Learning算法原理与实现方法。<br>
3.对比传统搜索算法与深度强化学习在迷宫问题中的效果与性能差异。<br>
4.培养独立解决问题的能力以及算法在现实问题中的应用能力。</p>
<h3 id="二-实验内容">二、实验内容 </h3>
<p>1.迷宫环境搭建</p>
<ul>
<li>使用Python构建迷宫地图（可采用二维数组模拟）。</li>
<li>设置起点、终点、障碍物等元素。</li>
</ul>
<p>2.DFS算法实现</p>
<ul>
<li>使用递归或栈实现深度优先遍历。</li>
<li>路径回溯，记录访问路径。</li>
<li>输出一条可行路径（非最优）。</li>
</ul>
<p>3.Deep Q-Learning实现</p>
<ul>
<li>构建状态空间与动作空间。</li>
<li>定义奖励函数（如到达终点+400，撞墙-10，每步-1等）。</li>
<li>构建神经网络拟合Q值函数。</li>
<li>训练智能体在迷宫中自主学习，优化策略。</li>
<li>多次训练后，观察路径选择是否趋于稳定与最优。</li>
</ul>
<h3 id="三-实现过程与代码">三、实现过程与代码 </h3>
<p><strong>创建迷宫：</strong> 利用系统提供的Maze类可以随机创建一个指定大小的迷宫(其中maze_size为迷宫大小)，利用print函数可以输出迷宫的size以及画出迷宫图</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-from">from</span> Maze <span class="token keyword keyword-import">import</span> Maze
maze <span class="token operator">=</span> Maze<span class="token punctuation">(</span>maze_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment"># 随机生成迷宫</span>
</code></pre><br>
<p><strong>基础算法实现：</strong> 我们选用了<strong>深度优先搜索算法</strong>作为迷宫的寻路算法，DFS算法的基本思路是：从起点出发，沿着一条路径向前走，直到走到死胡同为止，然后回溯到上一个节点，继续沿着另一条路径向前走，直到找到终点为止。这刚好对应栈的特性：后进先出，于是我们使用栈来实现深度优先搜索算法。</p>
<p>首先我们引用了SearchTree 类，用于表示搜索树的节点。每个节点包含以下属性：</p>
<ul>
<li>loc：当前节点的位置</li>
<li>to_this_action：到达当前节点的动作</li>
<li>parent：当前节点的父节点</li>
<li>children：当前节点的子节点列表</li>
</ul>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-class">class</span> <span class="token class-name">SearchTree</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> parent<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>loc <span class="token operator">=</span> loc  <span class="token comment"># 当前节点位置</span>
        self<span class="token punctuation">.</span>to_this_action <span class="token operator">=</span> action  <span class="token comment"># 到达当前节点的动作</span>
        self<span class="token punctuation">.</span>parent <span class="token operator">=</span> parent  <span class="token comment"># 当前节点的父节点</span>
        self<span class="token punctuation">.</span>children <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 当前节点的子节点</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">add_child</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> child<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>children<span class="token punctuation">.</span>append<span class="token punctuation">(</span>child<span class="token punctuation">)</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">is_leaf</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword keyword-return">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>children<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span>
</code></pre><p>接着我们定义了<strong>expand函数和back_propagation函数</strong>。</p>
<p>其中expand函数用于拓展当前节点，生成所有可能的子节点。该函数的基本思路是首先获取当前节点可以移动的方向。然后对于每个可以移动的方向，计算新位置。最后如果新位置未被访问过，则创建一个新的 SearchTree 节点，并将其添加到当前节点的子节点列表中</p>
<p>back_propagation函数用于从目标节点回溯到起始节点，记录路径。该函数的基本思路是先从目标节点开始，沿着父节点回溯，直到到达起始节点，然后将路径中的每个动作记录下来，并按顺序返回</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">expand</span><span class="token punctuation">(</span>maze<span class="token punctuation">,</span> is_visit_m<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
    can_move <span class="token operator">=</span> maze<span class="token punctuation">.</span>can_move_actions<span class="token punctuation">(</span>node<span class="token punctuation">.</span>loc<span class="token punctuation">)</span>
    <span class="token keyword keyword-for">for</span> a <span class="token keyword keyword-in">in</span> can_move<span class="token punctuation">:</span>
        new_loc <span class="token operator">=</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>node<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> move_map<span class="token punctuation">[</span>a<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword keyword-for">for</span> i <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword keyword-if">if</span> <span class="token keyword keyword-not">not</span> is_visit_m<span class="token punctuation">[</span>new_loc<span class="token punctuation">]</span><span class="token punctuation">:</span>
            child <span class="token operator">=</span> SearchTree<span class="token punctuation">(</span>loc<span class="token operator">=</span>new_loc<span class="token punctuation">,</span> action<span class="token operator">=</span>a<span class="token punctuation">,</span> parent<span class="token operator">=</span>node<span class="token punctuation">)</span>
            node<span class="token punctuation">.</span>add_child<span class="token punctuation">(</span>child<span class="token punctuation">)</span>


<span class="token keyword keyword-def">def</span> <span class="token function">back_propagation</span><span class="token punctuation">(</span>node<span class="token punctuation">)</span><span class="token punctuation">:</span>
    path <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword keyword-while">while</span> node<span class="token punctuation">.</span>parent <span class="token keyword keyword-is">is</span> <span class="token keyword keyword-not">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        path<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> node<span class="token punctuation">.</span>to_this_action<span class="token punctuation">)</span>
        node <span class="token operator">=</span> node<span class="token punctuation">.</span>parent
    <span class="token keyword keyword-return">return</span> path
</code></pre><p>然后我们实现了深度优先搜索算法的<strong>核心函数my_search</strong>。该函数的基本思路为首先获取机器人当前位置作为搜索树的根节点，并使用栈结构存储待探索的节点。算法维护一个与迷宫大小相同的访问矩阵，记录每个位置是否被访问过。每次从栈中取出当前节点，标记为已访问；若该节点位置为终点，则调用回溯函数重建路径并结束搜索；若当前节点为叶节点，则调用扩展函数生成其所有可行的子节点（未被访问的相邻位置）；这些子节点被逆序压入栈中，确保深度优先地继续探索。整个过程持续进行，直到找到通向终点的路径或栈为空为止</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">my_search</span><span class="token punctuation">(</span>maze<span class="token punctuation">)</span><span class="token punctuation">:</span>
    start <span class="token operator">=</span> maze<span class="token punctuation">.</span>sense_robot<span class="token punctuation">(</span><span class="token punctuation">)</span>
    root <span class="token operator">=</span> SearchTree<span class="token punctuation">(</span>loc<span class="token operator">=</span>start<span class="token punctuation">)</span>
    stack <span class="token operator">=</span> <span class="token punctuation">[</span>root<span class="token punctuation">]</span> 
    h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> _ <span class="token operator">=</span> maze<span class="token punctuation">.</span>maze_data<span class="token punctuation">.</span>shape
    is_visit_m <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">)</span>  
    path <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  

    <span class="token keyword keyword-while">while</span> stack<span class="token punctuation">:</span>
        current_node <span class="token operator">=</span> stack<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span> 
        is_visit_m<span class="token punctuation">[</span>current_node<span class="token punctuation">.</span>loc<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>  

        <span class="token keyword keyword-if">if</span> current_node<span class="token punctuation">.</span>loc <span class="token operator">==</span> maze<span class="token punctuation">.</span>destination<span class="token punctuation">:</span>  
            path <span class="token operator">=</span> back_propagation<span class="token punctuation">(</span>current_node<span class="token punctuation">)</span>
            <span class="token keyword keyword-break">break</span>

        <span class="token keyword keyword-if">if</span> current_node<span class="token punctuation">.</span>is_leaf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            expand<span class="token punctuation">(</span>maze<span class="token punctuation">,</span> is_visit_m<span class="token punctuation">,</span> current_node<span class="token punctuation">)</span>

        <span class="token comment"># 将子节点按逆序压入栈中</span>
        <span class="token keyword keyword-for">for</span> child <span class="token keyword keyword-in">in</span> <span class="token builtin">reversed</span><span class="token punctuation">(</span>current_node<span class="token punctuation">.</span>children<span class="token punctuation">)</span><span class="token punctuation">:</span>
            stack<span class="token punctuation">.</span>append<span class="token punctuation">(</span>child<span class="token punctuation">)</span>
    <span class="token keyword keyword-return">return</span> path
</code></pre><br>
<p><strong>强化学习算法：</strong> 我们选用了deep Q-learning算法作为迷宫的寻路算法，DQN算法的基本思路是：通过神经网络来近似Q-learning算法中的Q值函数，从而实现对动作价值的估计。因此要了解deep Q-learning算法，首先需要了解Q-learning算法。<br>
<strong>Q-Learning</strong> 是一个<strong>值迭代算法</strong>。与策略迭代算法不同，值迭代算法会计算每个”状态“或是”状态-动作“的值或是效用，然后在执行动作的时候，会设法最大化这个值。</p>
<p>在实际的执行过程中，Q-learning 算法将状态和动作构建成一张 Q_table 表来存储 Q 值，Q 表的行代表状态，列代表动作，表格中的每个元素代表在某个状态下执行某个动作的价值。Q-learning 算法的核心思想是通过不断地更新 Q 表来学习最优策略。具体来说，Q-learning 算法会根据当前状态和动作的 Q 值来计算一个新的 Q 值，然后将这个新值更新到 Q 表中。这个新值是通过以下公式计算的：<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><mo stretchy="false">[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>a</mi></munder><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Q(s,a) = (1-\alpha)Q(s,a) + \alpha [r + \gamma \max_{a} Q(s',a)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.5019em;vertical-align:-0.7em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.4em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)]</span></span></span></span></span><br>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span>代表对于当前的状态s，执行动作a后，获得的环境奖励，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>a</mi></msub><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\max_{a} Q(s',a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span>表示执行动作a到达下一个状态s'后，所有可能的动作中Q值最大的动作，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span>是折扣因子。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>是松弛因子，目的是使得的Q值更平滑。</p>
<p>然而，在初步的学习中，Q 值是不准确的，如果在这个时候都按照 Q 值来选择，那么会造成错误，而且在学习一段时间后，机器人的路线会相对固定，则机器人无法对环境进行有效的探索。</p>
<p>为了解决这个问题，我们需要在选择动作时引入一定的随机性。我们可以使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span>-greedy 策略来平衡探索和利用：以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 的概率随机选择一个动作，以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">1-\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 的概率选择当前 Q 值最大的动作。随着训练的进行，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 会逐渐减小，从而使得机器人在初期能够探索更多的状态，而在后期则能够更好地利用已经学到的知识。</p>
<p><strong>Deep Q-Learning算法</strong>在Q-learning算法的基础上，使用深度神经网络来近似 Q 值函数。随着迷宫规模的增大，Q 表的大小也会随之增大，导致 Q 表的存储和更新变得非常困难。因此，我们使用深度神经网络来近似 Q 值函数，目标是通过训练网络，让它输出每个动作在当前状态下的 Q 值，从而选择最优动作<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>≈</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>a</mi><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>a</mi></munder><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
             Q(s,a,\theta)  \approx Q(s,a) \\
     a = \arg\max_{a} Q(s,a,\theta) \\
    \end{align*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.34em;vertical-align:-1.42em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.92em;"><span style="top:-4.08em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span><span style="top:-2.58em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.4em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.42em;"><span></span></span></span></span></span></span></span></span></span></span></span><br>
为了解决训练不稳定、收敛困难等问题，DQN 引入了以下两个核心机制:</p>
<ul>
<li>经验回放：将智能体的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>r</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(s,a,r,s')</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>存储到一个经验池中，每次训练时从中随机采样小批量数据，从而打破数据之间的相关性，提高训练的稳定性。</li>
<li>目标网络：使用两个神经网络，一个是当前网络Q(s,a,θ)，另一个是目标网络Q(s,a,θ')，目标网络的参数每隔一段时间从当前网络拷贝，用于计算目标 Q 值。这样可以避免 Q 值的快速变化导致训练不稳定。<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>a</mi></munder><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>θ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex">Q = r + \gamma \max_{a} Q(s',a,\theta') \\</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.5019em;vertical-align:-0.7em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.4em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span></span></span></span></li>
</ul>
<p>实际的训练流程是按照下图进行的：</p>
<div align="center">
<img src="image.png" width="60%">
</div>
<ul>
<li>初始化Q网络和目标网络的参数，初始化经验池</li>
<li>在每一个时间步，执行以下操作：
<ul>
<li>选择动作：根据当前状态s和Q网络的参数θ，使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span>-greedy策略选择动作a。</li>
<li>执行动作：在环境中执行动作a，获得奖励r和下一个状态s'。</li>
<li>存储经验：将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>r</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(s,a,r,s')</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>存储到经验池中。</li>
<li>采样经验：从经验池中随机采样小批量数据<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>r</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(s,a,r,s')</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</li>
<li>更新Q网络：使用小批量数据计算目标 Q 值，最小化均方误差损失并更新 Q 网络的参数θ。</li>
<li>更新目标网络：每隔若干步，将当前网络的参数θ拷贝到目标网络的参数θ'。</li>
</ul>
</li>
</ul>
<p>示例的代码中已经提供了QRobot类和Runner类，其中QRobot类实现了Q表迭代和机器人动作的选择策略，Runner类用于机器人的训练（即不断迭代更新Q值）和可视化</p>
<p>我们自己定义的DQNRobot继承了QRobot类，参考了示例代码中提供的MinDQNRobot，在__init__函数中初始化了一些参数，同时额外实现了神经网络的定义，训练函数和参数更新函数，在实际训练过程中，我们调用runner类的train函数来进行训练，train函数会不断调用Robot类中的train_update函数来进行参数更新。具体代码如下：</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-class">class</span> <span class="token class-name">Robot</span><span class="token punctuation">(</span>QRobot<span class="token punctuation">)</span><span class="token punctuation">:</span>

    valid_action <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'u'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> <span class="token string">'d'</span><span class="token punctuation">,</span> <span class="token string">'l'</span><span class="token punctuation">]</span>

    <span class="token triple-quoted-string string">''' QLearning parameters'''</span>
    epsilon0 <span class="token operator">=</span> <span class="token number">0.5</span>  <span class="token comment"># 初始贪心算法探索概率</span>
    gamma <span class="token operator">=</span> <span class="token number">0.94</span>  <span class="token comment"># 公式中的 γ</span>

    EveryUpdate <span class="token operator">=</span> <span class="token number">1</span>  <span class="token comment"># the interval of target model's updating</span>

    <span class="token triple-quoted-string string">"""some parameters of neural network"""</span>
    target_model <span class="token operator">=</span> <span class="token boolean">None</span>
    eval_model <span class="token operator">=</span> <span class="token boolean">None</span>
    batch_size <span class="token operator">=</span> <span class="token number">32</span>
    learning_rate <span class="token operator">=</span> <span class="token number">1e-2</span>
    TAU <span class="token operator">=</span> <span class="token number">1e-3</span>
    step <span class="token operator">=</span> <span class="token number">1</span>  <span class="token comment"># 记录训练的步数</span>

    <span class="token triple-quoted-string string">"""setting the device to train network"""</span>
    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span> <span class="token keyword keyword-if">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword keyword-else">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> maze<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化 Robot 类
        :param maze:迷宫对象
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Robot<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>maze<span class="token punctuation">)</span>
        maze<span class="token punctuation">.</span>set_reward<span class="token punctuation">(</span>reward<span class="token operator">=</span><span class="token punctuation">{</span>
            <span class="token string">"hit_wall"</span><span class="token punctuation">:</span> <span class="token number">10.</span><span class="token punctuation">,</span>
            <span class="token string">"destination"</span><span class="token punctuation">:</span> <span class="token operator">-</span>maze<span class="token punctuation">.</span>maze_size <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token number">5.</span><span class="token punctuation">,</span>
            <span class="token string">"default"</span><span class="token punctuation">:</span> <span class="token number">1.</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>maze <span class="token operator">=</span> maze
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> <span class="token number">0</span>
        self<span class="token punctuation">.</span>maze_size <span class="token operator">=</span> maze<span class="token punctuation">.</span>maze_size

        <span class="token triple-quoted-string string">"""build network"""</span>
        self<span class="token punctuation">.</span>target_model <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>eval_model <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>_build_network<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""create the memory to store data"""</span>
        max_size <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>maze_size <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1e4</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>memory <span class="token operator">=</span> ReplayDataSet<span class="token punctuation">(</span>max_size<span class="token operator">=</span>max_size<span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""开启金手指，获取全图视野"""</span>
        self<span class="token punctuation">.</span>memory<span class="token punctuation">.</span>build_full_view<span class="token punctuation">(</span>maze<span class="token operator">=</span>maze<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>loss_list <span class="token operator">=</span> self<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword keyword-def">def</span> <span class="token function">_build_network</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        seed <span class="token operator">=</span> <span class="token number">0</span>
        random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""build target model"""</span>
        self<span class="token punctuation">.</span>target_model <span class="token operator">=</span> QNetwork<span class="token punctuation">(</span>state_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> action_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> seed<span class="token operator">=</span>seed<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""build eval model"""</span>
        self<span class="token punctuation">.</span>eval_model <span class="token operator">=</span> QNetwork<span class="token punctuation">(</span>state_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> action_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> seed<span class="token operator">=</span>seed<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""build the optimizer"""</span>
        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>eval_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>self<span class="token punctuation">.</span>learning_rate<span class="token punctuation">)</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">target_replace_op</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
            Soft update the target model parameters.
            θ_target = τ*θ_local + (1 - τ)*θ_target
        """</span>
        <span class="token triple-quoted-string string">""" replace the whole parameters"""</span>
        self<span class="token punctuation">.</span>target_model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>self<span class="token punctuation">.</span>eval_model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">_choose_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword keyword-if">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>epsilon<span class="token punctuation">:</span>
            action <span class="token operator">=</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>self<span class="token punctuation">.</span>valid_action<span class="token punctuation">)</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>eval_model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword keyword-with">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                q_next <span class="token operator">=</span> self<span class="token punctuation">.</span>eval_model<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># use target model choose action</span>
            self<span class="token punctuation">.</span>eval_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

            action <span class="token operator">=</span> self<span class="token punctuation">.</span>valid_action<span class="token punctuation">[</span>np<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>q_next<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token keyword keyword-return">return</span> action

    <span class="token keyword keyword-def">def</span> <span class="token function">_learn</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword keyword-if">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>memory<span class="token punctuation">)</span> <span class="token operator">&lt;</span> batch<span class="token punctuation">:</span>
            <span class="token keyword keyword-print">print</span><span class="token punctuation">(</span><span class="token string">"the memory data is not enough"</span><span class="token punctuation">)</span>
            <span class="token keyword keyword-return">return</span>
        state<span class="token punctuation">,</span> action_index<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> is_terminal <span class="token operator">=</span> self<span class="token punctuation">.</span>memory<span class="token punctuation">.</span>random_sample<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">""" convert the data to tensor type"""</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        action_index <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>action_index<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        reward <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>reward<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        next_state <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>next_state<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        is_terminal <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>is_terminal<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>eval_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>target_model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""Get max predicted Q values (for next states) from target model"""</span>
        Q_targets_next <span class="token operator">=</span> self<span class="token punctuation">.</span>target_model<span class="token punctuation">(</span>next_state<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""Compute Q targets for current states"""</span>
        Q_targets <span class="token operator">=</span> reward <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> Q_targets_next <span class="token operator">*</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>is_terminal<span class="token punctuation">)</span> <span class="token operator">-</span> is_terminal<span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""Get expected Q values from local model"""</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        Q_expected <span class="token operator">=</span> self<span class="token punctuation">.</span>eval_model<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>action_index<span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""Compute loss"""</span>
        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>Q_expected<span class="token punctuation">,</span> Q_targets<span class="token punctuation">)</span>
        loss_item <span class="token operator">=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">""" Minimize the loss"""</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""copy the weights of eval_model to the target_model"""</span>
        self<span class="token punctuation">.</span>target_replace_op<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword keyword-return">return</span> loss_item
    <span class="token keyword keyword-def">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        batch_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>memory<span class="token punctuation">)</span>
        
        <span class="token comment"># 训练，直到能走出这个迷宫</span>
        <span class="token keyword keyword-while">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_learn<span class="token punctuation">(</span>batch<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
            loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
            success <span class="token operator">=</span> <span class="token boolean">False</span>
            self<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword keyword-for">for</span> _ <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>maze<span class="token punctuation">.</span>maze_size <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                a<span class="token punctuation">,</span> r <span class="token operator">=</span> self<span class="token punctuation">.</span>test_update<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment">#     print("action:", a, "reward:", r)</span>
                <span class="token keyword keyword-if">if</span> r <span class="token operator">==</span> self<span class="token punctuation">.</span>maze<span class="token punctuation">.</span>reward<span class="token punctuation">[</span><span class="token string">"destination"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                    <span class="token keyword keyword-return">return</span> loss_list
            
    <span class="token keyword keyword-def">def</span> <span class="token function">train_update</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> self<span class="token punctuation">.</span>sense_state<span class="token punctuation">(</span><span class="token punctuation">)</span>
        action <span class="token operator">=</span> self<span class="token punctuation">.</span>_choose_action<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
        reward <span class="token operator">=</span> self<span class="token punctuation">.</span>maze<span class="token punctuation">.</span>move_robot<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
        <span class="token keyword keyword-return">return</span> action<span class="token punctuation">,</span> reward
    
    <span class="token keyword keyword-def">def</span> <span class="token function">test_update</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sense_state<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int16<span class="token punctuation">)</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>eval_model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword keyword-with">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            q_value <span class="token operator">=</span> self<span class="token punctuation">.</span>eval_model<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

        action <span class="token operator">=</span> self<span class="token punctuation">.</span>valid_action<span class="token punctuation">[</span>np<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>q_value<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        reward <span class="token operator">=</span> self<span class="token punctuation">.</span>maze<span class="token punctuation">.</span>move_robot<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
        <span class="token keyword keyword-return">return</span> action<span class="token punctuation">,</span> reward
</code></pre><h3 id="四-实验结果与分析">四、实验结果与分析 </h3>
<p><strong>dfs算法结果：</strong> 首先我们对基础搜索算法dfs进行测试，实验结果表明，对于20*20的迷宫，DFS算法能够在较短的时间（1s内）找到一条可行路径。</p>
<div align="center">
<img src="image-1.png" width="50%">
</div>
<p><strong>DQN算法结果：</strong><br>
接着我们对深度强化学习算法DQN进行测试，实验结果表明，当迷宫规模较小时（如5×5时），DQN算法可以找到一条可行路径，但在迷宫规模较大时（如10×10时），DQN算法往往陷入重复的循环中，无法找到一条可行路径。</p>
<div style="display: flex; justify-content: center;">
  <img src="image-2.png" style="width: 50%; margin-right: 10px;">
  <img src="image-3.png" style="width: 45%;">
</div>
<p>起初我们认为是训练次数epoch=10太小导致的，但当我们将epoch增大到50，100时，仍然会陷入左右或上下原地踏步的循环中，于是我们继续对代码进行分析与调试，发现了两个可能原因：</p>
<ul>
<li>奖励函数中对目标点的奖励设置过低，只有-50，而撞墙的惩罚设置过高，达到10，所以，机器人学习到的策略可能是：不要撞墙就好，其他都差不多，但不会强烈趋向目标，所以就会陷入原地踏步的死循环中</li>
<li>前期的动作基本是随机乱走，经验池质量较差以及经验同质化严重，全是“撞墙”、“兜圈子”数据，模型学不到什么策略，这就导致了Q网络学不到正确方向，陷入局部最优甚至死循环。</li>
</ul>
<p>针对这两个问题，我们进行了如下修改：</p>
<p>首先我们将目标函数的奖励设置为随迷宫的大小而变化，这样当迷宫规模扩大时，目标点的奖励也会相应增大，从而使得机器人更倾向于朝着目标点移动。</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>maze<span class="token punctuation">.</span>set_reward<span class="token punctuation">(</span>reward<span class="token operator">=</span><span class="token punctuation">{</span>
            <span class="token string">"hit_wall"</span><span class="token punctuation">:</span> <span class="token number">10.</span><span class="token punctuation">,</span>
            <span class="token string">"destination"</span><span class="token punctuation">:</span> <span class="token operator">-</span>maze<span class="token punctuation">.</span>maze_size <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token number">4.</span><span class="token punctuation">,</span>
            <span class="token string">"default"</span><span class="token punctuation">:</span> <span class="token number">1.</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre><p>然后，我们在创建robot实例时加入预训练函数，强制 DQN 至少找到过一次成功路径，reward["destination"] 被引入经验池，这样模型参数已经能“部分感知”奖励更大的路径方向，后续训练的 ε-greedy 才能探索到更有价值的动作</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    batch_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>memory<span class="token punctuation">)</span>
    
    <span class="token comment"># 训练，直到能走出这个迷宫</span>
    <span class="token keyword keyword-while">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_learn<span class="token punctuation">(</span>batch<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
        loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
        success <span class="token operator">=</span> <span class="token boolean">False</span>
        self<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword keyword-for">for</span> _ <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>maze<span class="token punctuation">.</span>maze_size <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            a<span class="token punctuation">,</span> r <span class="token operator">=</span> self<span class="token punctuation">.</span>test_update<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#     print("action:", a, "reward:", r)</span>
            <span class="token keyword keyword-if">if</span> r <span class="token operator">==</span> self<span class="token punctuation">.</span>maze<span class="token punctuation">.</span>reward<span class="token punctuation">[</span><span class="token string">"destination"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                <span class="token keyword keyword-return">return</span> loss_list
</code></pre><p>修改完后，我们对10×10的迷宫进行测试，发现DQN算法能够稳定地找到一条可行路径。</p>
<div align="center">
<img src="image-4.png" width="50%">
</div>
最后我们进行系统测试，结果表明对于3×3，5×5，11×11的迷宫，DQN算法都能够找到一条可行路径。
<div align="center">
<img src="image-5.png" width="50%">
</div>
<div align="center">
<img src="image-6.png" width="50%">
</div>
<div align="center">
<img src="image-7.png" width="50%">
</div>
<div align="center">
<img src="image-8.png" width="50%">
</div>
<h3 id="五-实验总结">五、实验总结 </h3>
<p>本次实验通过将传统的深度优先搜索算法与现代的Deep Q-Learning深度强化学习算法相结合，对比分析了它们在迷宫导航任务中的表现与适用性。DFS提供了一个基础、确定性强但灵活性不足的路径搜索方法，而Deep Q-Learning则展示了强大的自适应能力和学习能力，尤其在复杂环境下具有显著优势。</p>
<p>通过动手实现这两种算法，不仅加深了我们对路径规划问题求解策略的理解，也锻炼了我们在实际工程中调试、建模和优化算法的能力。实验进一步让我们认识到，虽然强化学习在训练初期可能面临探索不足或收敛困难等挑战，但其最终表现出的智能决策能力为今后机器人自主导航技术的研究与应用提供了重要方向。</p>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>